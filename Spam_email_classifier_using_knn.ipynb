{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPvuRPtAoNj6/xl0QMB/P7D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ak4721269/spam-email-classifier/blob/main/Spam_email_classifier_using_knn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install the required packages"
      ],
      "metadata": {
        "id": "N9nr86X-Lw71"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FDg9SaTJX_-z"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcReE1-KOcxE",
        "outputId": "16b6d2b2-26d6-49a0-8a82-d75ecf4e3705"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the zip file of dataset and unzip it by running it below cell."
      ],
      "metadata": {
        "id": "CVSfvPmGYep-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "import os\n",
        "\n",
        "# Specify the path to your .tar.gz file\n",
        "tar_file_path = '/content/enron1.tar.gz'\n",
        "\n",
        "# Specify the path where you want to extract the files\n",
        "extract_path = '/content/dataset'\n",
        "\n",
        "# Create the extract folder if it doesn't exist\n",
        "if not os.path.exists(extract_path):\n",
        "    os.makedirs(extract_path)\n",
        "\n",
        "# Open the .tar.gz file\n",
        "with tarfile.open(tar_file_path, 'r:gz') as tar:\n",
        "    # Extract all contents to the specified folder\n",
        "    tar.extractall(path=extract_path)\n",
        "\n",
        "print(f\"Files extracted to {extract_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EM2y0qoydyqV",
        "outputId": "275c6cd2-a22a-4780-e364-694d9d601c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to /content/dataset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data():\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    ham_files_location = os.listdir(\"/content/dataset/enron1/ham\")\n",
        "    spam_files_location = os.listdir(\"/content/dataset/euron1/spam\")\n",
        "    data = []"
      ],
      "metadata": {
        "id": "jPjiXu8eYNhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def load_data():\n",
        "    print(\"Loading data...\")\n",
        "\n",
        "    data = []\n",
        "\n",
        "    # Load ham email\n",
        "    ham_path = \"/content/dataset/enron1/ham\"\n",
        "    for file_path in os.listdir(ham_path):\n",
        "        file_path = os.path.join(ham_path, file_path)\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "            text = f.read()\n",
        "            data.append([text, \"ham\"])\n",
        "\n",
        "    # Load spam email\n",
        "    spam_path = \"/content/dataset/enron1/spam\"\n",
        "    for file_path in os.listdir(spam_path):\n",
        "        file_path = os.path.join(spam_path, file_path)\n",
        "        with open(file_path, \"r\", encoding=\"latin-1\") as f:\n",
        "            text = f.read()\n",
        "            data.append([text, \"spam\"])\n",
        "\n",
        "    data = np.array(data)\n",
        "    print(\"flag 1: loaded data\")\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "Kn9RPl4EBmHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data):\n",
        "    print(\"Preprocessing data...\")\n",
        "\n",
        "    punc = string.punctuation           # Punctuation list\n",
        "    sw = stopwords.words('english')\n",
        "    for record in data:\n",
        "        # Remove common punctuation and symbols\n",
        "       for item in punc:\n",
        "           record[0] = record[0].replace(item, \"\")\n",
        "           # Lowercase all letters and remove stopwords\n",
        "           splittedWords = record[0].split()\n",
        "           newText = \"\"\n",
        "           for word in splittedWords:\n",
        "              if word not in sw:\n",
        "                  word = word.lower()\n",
        "                  newText = newText + \" \" + word\n",
        "           record[0] = newText\n",
        "\n",
        "    print(\"flag 2: preprocessed data\")\n",
        "    return data\n"
      ],
      "metadata": {
        "id": "b3xXw7IQC5wF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting original dataset into training dataset and test dataset\n",
        "def split_data(data):\n",
        "    print(\"Splitting data...\")\n",
        "\n",
        "    features = data[:, 0]   # array containing all email text bodies\n",
        "    labels = data[:, 1]     # array containing corresponding labels\n",
        "    print(labels)\n",
        "    training_data, test_data, training_labels, test_labels =\\\n",
        "        train_test_split(features, labels, test_size = 0.27, random_state = 42)\n",
        "\n",
        "    print(\"flag 3: splitted data\")\n",
        "    return training_data, test_data, training_labels, test_labels"
      ],
      "metadata": {
        "id": "VK9embfFKJDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_count(text):\n",
        "    wordCounts = dict()\n",
        "    for word in text.split():\n",
        "        if word in wordCounts:\n",
        "            wordCounts[word] += 1\n",
        "        else:\n",
        "            wordCounts[word] = 1\n",
        "\n",
        "    return wordCounts"
      ],
      "metadata": {
        "id": "BflPQm1HKZ-w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def euclidean_difference(test_WordCounts, training_WordCounts):\n",
        "    total = 0\n",
        "    for word in test_WordCounts:\n",
        "        if word in test_WordCounts and word in training_WordCounts:\n",
        "            total += (test_WordCounts[word] - training_WordCounts[word])**2\n",
        "            del training_WordCounts[word]\n",
        "        else:\n",
        "            total += test_WordCounts[word]**2\n",
        "    for word in training_WordCounts:\n",
        "            total += training_WordCounts[word]**2\n",
        "\n",
        "    return total**0.5\n"
      ],
      "metadata": {
        "id": "_o_0t8ebKffw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_class(selected_Kvalues):\n",
        "    spam_count = 0\n",
        "    ham_count = 0\n",
        "    for value in selected_Kvalues:\n",
        "      if value[0] == \"spam\":\n",
        "          spam_count += 1\n",
        "      else:\n",
        "          ham_count += 1\n",
        "\n",
        "    if spam_count > ham_count:\n",
        "        return \"spam\"\n",
        "    else:\n",
        "        return \"ham\""
      ],
      "metadata": {
        "id": "DicejNYEKklz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def knn_classifier(training_data, training_labels, test_data, K, tsize):\n",
        "    print(\"Running KNN Classifier...\")\n",
        "\n",
        "    result = []\n",
        "    counter = 1\n",
        "    # word counts for training email\n",
        "    training_WordCounts = []\n",
        "    for training_text in training_data:\n",
        "            training_WordCounts.append(get_count(training_text))\n",
        "    for test_text in test_data:\n",
        "        similarity = [] # List of euclidean distances\n",
        "        test_WordCounts = get_count(test_text)  # word counts for test email\n",
        "\n",
        "        # Getting euclidean difference\n",
        "        for index in range(len(training_data)):\n",
        "              euclidean_diff =\\\n",
        "                    euclidean_difference(test_WordCounts, training_WordCounts[index])\n",
        "              similarity.append([training_labels[index], euclidean_diff])\n",
        "    # Sort list in ascending order based on euclidean difference\n",
        "        similarity = sorted(similarity, key = lambda i:i[1])\n",
        "\n",
        "         # Select K nearest neighbours\n",
        "        selected_Kvalues = []\n",
        "        for i in range(K):\n",
        "            selected_Kvalues.append(similarity[i])\n",
        "\n",
        "    # Predicting the class of email\n",
        "        result.append(get_class(selected_Kvalues))\n",
        "    return result\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "n4zNv4efMLZU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(K):\n",
        "    data = load_data()\n",
        "    data = preprocess_data(data)\n",
        "    training_data, test_data, training_labels, test_labels = split_data(data)\n",
        "    tsize = len(test_data)\n",
        "    result = knn_classifier(training_data, training_labels, test_data[:tsize], K, tsize)\n",
        "    accuracy = accuracy_score(test_labels[:tsize], result)\n",
        "    print(\"training data size\\t: \" + str(len(training_data)))\n",
        "    print(\"test data size\\t\\t: \" + str(len(test_data)))\n",
        "    print(\"K value\\t\\t\\t\\t: \" + str(K))\n",
        "    print(\"Samples tested\\t\\t: \" + str(tsize))\n",
        "    print(\"% accuracy\\t\\t\\t: \" + str(accuracy * 100))\n",
        "    print(\"Number correct\\t\\t: \" + str(int(accuracy * tsize)))\n",
        "    print(\"Number wrong\\t\\t: \" + str(int((1 - accuracy) * tsize)))"
      ],
      "metadata": {
        "id": "tDXrOgb6N51g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main(11)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucTZ8GEGORHq",
        "outputId": "6cdbafbe-eadb-4097-8348-b935772f3950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data...\n",
            "flag 1: loaded data\n",
            "Preprocessing data...\n",
            "flag 2: preprocessed data\n",
            "Splitting data...\n",
            "['ham' 'ham' 'ham' ... 'spam' 'spam' 'spam']\n",
            "flag 3: splitted data\n",
            "Running KNN Classifier...\n",
            "training data size\t: 3775\n",
            "test data size\t\t: 1397\n",
            "K value\t\t\t\t: 11\n",
            "Samples tested\t\t: 1397\n",
            "% accuracy\t\t\t: 72.15461703650679\n",
            "Number correct\t\t: 1008\n",
            "Number wrong\t\t: 389\n"
          ]
        }
      ]
    }
  ]
}